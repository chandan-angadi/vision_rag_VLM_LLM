{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Vision Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline for technical documentation using:\n",
    "- PDF text and image extraction\n",
    "- Image description generation\n",
    "- Semantic search\n",
    "- Context-aware response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Dependencies\n",
    "We'll need several libraries for our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install pymupdf pytesseract langchain-community faiss-cpu ollama pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "# Langchain and ML imports\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "PDF_PATH = \"data/project_doc.pdf\"\n",
    "OUTPUT_DIR = \"data/extracted_images\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model Configuration\n",
    "EMBEDDING_MODEL = \"llama3.2\"\n",
    "VISION_MODEL = \"llama3.2-vision\"\n",
    "LLM_MODEL = \"llama3.2\"\n",
    "\n",
    "# Initialize models\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "vision_llm = Ollama(model=VISION_MODEL)\n",
    "llm = Ollama(model=LLM_MODEL)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, \n",
    "    chunk_overlap=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Extraction and Description Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    Extract images from PDF using PyMuPDF\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF file\n",
    "        output_dir (str): Directory to save extracted images\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted image paths\n",
    "    \"\"\"\n",
    "    # Open the PDF document\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_images = []\n",
    "\n",
    "    # Iterate through each page of the PDF\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        images = page.get_images(full=True)\n",
    "        \n",
    "        # Process each image in the page\n",
    "        for img_index, img_info in enumerate(images):\n",
    "            try:\n",
    "                xref = img_info[0]\n",
    "                # Create a Pixmap object from the xref\n",
    "                pixmap = fitz.Pixmap(doc, xref)\n",
    "                \n",
    "                # Check if the pixmap has valid data\n",
    "                if pixmap.width == 0 or pixmap.height == 0 or not pixmap.samples:\n",
    "                    print(f\"Skipping invalid image at page {page_num+1}, index {img_index}\")\n",
    "                    continue\n",
    "\n",
    "                # If the image is in grayscale or other format, convert to RGB\n",
    "                if pixmap.n > 4:  # CMYK or similar\n",
    "                    pixmap = fitz.Pixmap(fitz.csRGB, pixmap)\n",
    "                \n",
    "                # Save the image to a PIL Image\n",
    "                img = Image.frombytes(\n",
    "                    \"RGB\", \n",
    "                    [pixmap.width, pixmap.height], \n",
    "                    pixmap.samples\n",
    "                )\n",
    "                \n",
    "                # Save the image to the output directory\n",
    "                img_path = os.path.join(\n",
    "                    output_dir, \n",
    "                    f\"page_{page_num+1}_image_{img_index}.png\"\n",
    "                )\n",
    "                img.save(img_path)\n",
    "                extracted_images.append(img_path)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image at page {page_num+1}, index {img_index}: {e}\")\n",
    "    \n",
    "    return extracted_images\n",
    "\n",
    "\n",
    "def describe_image(image_path, vision_llm):\n",
    "    \"\"\"\n",
    "    Generate detailed description for an image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        vision_llm: Vision language model name\n",
    "    \n",
    "    Returns:\n",
    "        str: Image description\n",
    "    \"\"\"\n",
    "    print(image_path)\n",
    "    # Use Ollama to analyze the image with Llama 3.2-Vision\n",
    "    response = ollama.chat(\n",
    "        model=vision_llm,\n",
    "        messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Analyze this technical documentation image in detail where don't mention the color of charts in the image focus more on content.\",\n",
    "        \"images\": [image_path]\n",
    "        }],\n",
    "    )\n",
    "\n",
    "    # Extract the model's response about the image\n",
    "    cleaned_text = response['message']['content'].strip()\n",
    "    \n",
    "    return clean_text_general(cleaned_text)\n",
    "\n",
    "# Function to clean and structure the data\n",
    "def clean_text_general(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans and structures raw text data for various use cases, including vector store preparation.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Raw input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and structured text entries as a single string separated by newlines.\n",
    "    \"\"\"\n",
    "    # Normalize whitespace\n",
    "    normalized_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "\n",
    "    # Remove consecutive stars (**) in a sentence\n",
    "    normalized_text = re.sub(r'\\*{2,}', '', normalized_text)\n",
    "\n",
    "    # Split into lines and remove unnecessary empty lines or excessive whitespace\n",
    "    lines = normalized_text.split('.')\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Join the cleaned lines into a single string separated by newlines\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Content Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path, output_dir, vision_llm):\n",
    "    \"\"\"\n",
    "    Extract text and generate image descriptions\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF file\n",
    "        output_dir (str): Directory to save images\n",
    "        vision_llm: Vision language model\n",
    "    \n",
    "    Returns:\n",
    "        List of page contents with text and image descriptions\n",
    "    \"\"\"\n",
    "    # Extract images\n",
    "    image_paths = extract_images_from_pdf(pdf_path, output_dir)\n",
    "    \n",
    "    # Describe images\n",
    "    image_descriptions = {}\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            # Convert the response to a JSON-compatible string if necessary\n",
    "            description = describe_image(path, vision_llm)\n",
    "            print(description)\n",
    "            if isinstance(description, bytes):\n",
    "                description = description.decode(\"utf-8\")  # Convert bytes to string\n",
    "            image_descriptions[path] = description\n",
    "        except Exception as e:\n",
    "            print(f\"Error describing image {path}: {e}\")\n",
    "            image_descriptions[path] = \"Error generating description.\"\n",
    "\n",
    "    # Extract PDF text\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page_contents = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        page_text = page.get_text()\n",
    "        \n",
    "        # Add image descriptions for this page\n",
    "        page_images_desc = [\n",
    "            desc for path, desc in image_descriptions.items()\n",
    "            if f\"page_{page_num+1}_\" in path\n",
    "        ]\n",
    "        \n",
    "        if page_images_desc:\n",
    "            page_text += \"\\n\\n--- Image Descriptions ---\\n\"\n",
    "            page_text += \"\\n\".join(page_images_desc)\n",
    "        \n",
    "        page_contents.append({\n",
    "            'page_content': page_text,\n",
    "            'page_number': page_num + 1\n",
    "        })\n",
    "    \n",
    "    return page_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store and Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(page_contents, embeddings, text_splitter):\n",
    "    \"\"\"\n",
    "    Create FAISS vector store from PDF contents\n",
    "    \n",
    "    Args:\n",
    "        page_contents (List[Dict]): Extracted page contents\n",
    "        embeddings: Embedding model\n",
    "        text_splitter: Text splitting utility\n",
    "    \n",
    "    Returns:\n",
    "        FAISS vector store\n",
    "    \"\"\"\n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.create_documents(\n",
    "        [doc['page_content'] for doc in page_contents],\n",
    "        metadatas=[{'page_number': doc['page_number']} for doc in page_contents]\n",
    "    )\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    return FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "def retrieve_context(vector_store, query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant context from vector store\n",
    "    \n",
    "    Args:\n",
    "        vector_store: FAISS vector store\n",
    "        query (str): Search query\n",
    "        k (int): Number of top results\n",
    "    \n",
    "    Returns:\n",
    "        Retrieved context documents\n",
    "    \"\"\"\n",
    "    return vector_store.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/extracted_images/page_1_image_0.png\n",
      "The provided image is a flowchart that illustrates the process of machine learning development and deployment, from data collection to model training and testing\n",
      "Step 1: Data Collection * The first step in the process is collecting relevant data\n",
      "* This can be done through various means such as web scraping, APIs, or manual entry\n",
      "* The collected data is then stored in a database or data warehouse for further processing\n",
      "Step 2: Feature Engineering * Once the data is collected, it needs to be preprocessed and transformed into a format suitable for machine learning algorithms\n",
      "* This involves tasks such as feature scaling, normalization, and encoding categorical variables\n",
      "* The resulting features are then stored in a separate dataset or database\n",
      "Step 3: Model Training * With the preprocessed data, the next step is to train a machine learning model using one of several popular algorithms such as linear regression, decision trees, random forests, support vector machines (SVMs), k-nearest neighbors (KNN), and neural networks\n",
      "* The choice of algorithm depends on the nature of the problem being addressed\n",
      "Step 4: Model Evaluation * After training a model, it's essential to evaluate its performance using metrics such as accuracy, precision, recall, F1 score, mean squared error (MSE), R-squared, and mean absolute error (MAE)\n",
      "* These metrics help determine how well the model performs on unseen data\n",
      "Step 5: Model Deployment * Once a model is trained and evaluated, it's ready for deployment in a production environment\n",
      "* This involves integrating the model with existing applications or systems using APIs or other interfaces\n",
      "* The deployed model can be used to make predictions or classify new data points\n",
      "Step 6: Monitoring and Maintenance * After deploying a model, it's crucial to monitor its performance over time and address any issues that arise\n",
      "* This includes tracking metrics such as accuracy, precision, recall, F1 score, MSE, R-squared, and MAE to ensure the model remains effective\n",
      "Step 7: Model Updates * As new data becomes available or when existing data changes, it may be necessary to update the model to reflect these changes\n",
      "* This involves retraining the model using the updated data and redeploying it in the production environment\n",
      "In summary, the flowchart illustrates the key steps involved in developing and deploying a machine learning model\n",
      "It highlights the importance of data collection, feature engineering, model training, evaluation, deployment, monitoring, and maintenance to ensure that the model remains effective over time\n",
      "data/extracted_images/page_3_image_0.png\n",
      "The image presents a flowchart illustrating the PySpark UDF (User-Defined Function) process, which is a crucial component of Apache Spark's programming model\n",
      "The flowchart is divided into several sections, each representing a different stage in the UDF execution process\n",
      "* Driver Program * This section represents the driver program that initiates the UDF execution process\n",
      "* It contains two boxes labeled \"1 Driver program (Python)\" and \"2 SparkSession (JVM)\", indicating that the driver program is written in Python and uses the Java Virtual Machine (JVM) to execute the Spark session\n",
      "* Executor 1 * This section represents the first executor, which is responsible for executing the UDF code\n",
      "* It contains three boxes labeled \"3 Spark DataFrame (JVM)\", \"4\", and \"5 UDF (Python)\"\n",
      "* The box labeled \"3 Spark DataFrame (JVM)\" indicates that the executor receives a Spark DataFrame from the driver program, which is written in Java Virtual Machine (JVM) code\n",
      "* The box labeled \"4\" represents an unknown process or step in the UDF execution process\n",
      "* The box labeled \"5 UDF (Python)\" indicates that the executor executes the UDF code, which is written in Python\n",
      "* Executor 2 * This section represents the second executor, which is also responsible for executing the UDF code\n",
      "* It contains two boxes labeled \"3 Spark DataFrame (JVM)\" and \"5 UDF (Python)\"\n",
      "* The box labeled \"3 Spark DataFrame (JVM)\" indicates that the executor receives a Spark DataFrame from the driver program, which is written in Java Virtual Machine (JVM) code\n",
      "* The box labeled \"5 UDF (Python)\" indicates that the executor executes the UDF code, which is written in Python\n",
      "In summary, the flowchart illustrates the PySpark UDF process, which involves several stages: 1\n",
      "The driver program initiates the UDF execution process by creating a Spark session and sending it to one or more executors\n",
      "2\n",
      "The executor receives the Spark DataFrame from the driver program and executes the UDF code, which is written in Python\n",
      "Overall, the flowchart provides a clear visual representation of the PySpark UDF process, highlighting the key stages involved in executing user-defined functions within the Apache Spark ecosystem\n",
      "data/extracted_images/page_5_image_0.png\n",
      "The provided diagram illustrates a comprehensive workflow for machine learning model training and deployment, encompassing various stages from data preparation to model evaluation\n",
      "Data Preparation * Data Validation: The process ensures that the input data is accurate and consistent\n",
      "* Feature Engineering: This step involves transforming raw data into features suitable for analysis\n",
      "Model Training * Model Selection: Choosing an appropriate algorithm or architecture based on the problem's requirements\n",
      "* Hyperparameter Tuning: Fine-tuning model parameters to optimize performance\n",
      "* Training: Running the selected algorithm with the prepared data to generate a predictive model\n",
      "Model Evaluation * Model Performance Monitoring: Tracking key metrics such as accuracy, precision, recall, and F1-score to assess the model's effectiveness\n",
      "* Model Retraining: Periodically retraining the model using updated or additional data to maintain its performance over time\n",
      "Deployment and Maintenance * Model Deployment: Integrating the trained model into a production environment for real-time predictions\n",
      "* Continuous Monitoring: Regularly monitoring the model's performance and updating it as necessary to ensure ongoing accuracy\n",
      "This workflow provides a structured approach to machine learning model development, emphasizing data quality, algorithm selection, hyperparameter tuning, and continuous evaluation and improvement\n",
      "data/extracted_images/page_6_image_0.png\n",
      "The image depicts a flowchart illustrating the process of building, testing, and deploying software or applications\n",
      "* The flowchart consists of three stages: Start, Build, Test, and Deploy\n",
      "* Each stage is represented by a circle with an arrow pointing to the next stage\n",
      "* The arrows indicate the sequence in which these stages should be performed\n",
      "Starting from the leftmost circle labeled \"Start,\" we move clockwise through each subsequent stage until reaching the rightmost circle labeled \"Deploy\n",
      "\" * The first stage, \"Start,\" likely represents the initial planning and preparation phase for software development or deployment\n",
      "This could involve defining project goals, setting up necessary tools and infrastructure, and assigning tasks to team members\n",
      "Moving on to the second stage, \"Build,\" we enter the actual development phase where code is written, tested, and integrated into the application\n",
      "* During this stage, developers work on creating new features, fixing bugs, and refining existing functionality\n",
      "The third stage, \"Test,\" involves verifying that the built application meets requirements and functions as expected without errors or malfunctions\n",
      "This stage may include manual testing by quality assurance (QA) teams, automated testing using tools like unit tests or integration tests, and user acceptance testing (UAT)\n",
      "* Finally, in the last stage labeled \"Deploy,\" the tested application is prepared for release into production environments where it will be available to end-users\n",
      "Deployment might involve uploading the application to a server, configuring database connections, setting up monitoring tools, or performing other necessary setup tasks\n",
      "In summary, this flowchart outlines the key stages involved in developing and deploying software applications\n",
      "It emphasizes the importance of each stage in ensuring that the final product meets user needs, is free from bugs, and can be effectively maintained over time\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract PDF content with images\n",
    "page_contents = extract_pdf_content(PDF_PATH, OUTPUT_DIR, VISION_MODEL)\n",
    "    \n",
    "# Create vector store\n",
    "vector_store = create_vector_store(page_contents, embeddings, text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "    Context: {context}\n",
    "    \n",
    "    Query: {question}\n",
    "    You are an assistant that provides answers to questions based on\n",
    "    a given context incorporating any relevant details from the document, \n",
    "    including image descriptions, answer in a friendly way. \n",
    "\n",
    "    Answer the question based on the context. If you can't answer the\n",
    "    question, reply \"I don't know\".\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Block diagram of project architecture\n",
      "\n",
      "Response: Based on the provided context, I'll do my best to answer your question.\n",
      "\n",
      "The project architecture is described as an end-to-end solution for scalable and accurate predictions for product sales forecasting and scalability of application.\n",
      "\n",
      "A block diagram of the project architecture can be represented as follows:\n",
      "\n",
      "1. Data Ingestion:\n",
      "\t* New data is added to the repository.\n",
      "\t* `incremental_load.py` file sends an API request to the ML pipeline along with the new data.\n",
      "2. Data Validation:\n",
      "\t* The pipeline performs data validation steps based on the data validation report.\n",
      "3. Model Training and Deployment:\n",
      "\t* If the data passes validation, a new model is trained and deployed.\n",
      "4. Continuous Integration (CI) Framework:\n",
      "\t* The CI framework tests whether the development code should be pushed to production.\n",
      "\t* It includes unit tests and integration tests using pytest in Python.\n",
      "5. Prediction Consumption Pipeline:\n",
      "\t* The pipeline consumes predictions from the model.\n",
      "6. Model Performance Monitoring Framework:\n",
      "\t* The framework monitors the performance of the trained model.\n",
      "\n",
      "The block diagram can be represented as a flowchart, with each step connected to the next one:\n",
      "\n",
      "```\n",
      "                                      +---------------+\n",
      "                                      |  New Data   |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      | Incremental Load|\n",
      "                                      |  (incremental_load.py)|\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+       +---------------+\n",
      "                                      |  Data Validation|       |  Model Training  |\n",
      "                                      +---------------+       +---------------+\n",
      "                                             |                         |\n",
      "                                             |                         |\n",
      "                                             v                         v\n",
      "                                      +---------------+       +---------------+       +---------------+\n",
      "                                      | CI Framework   |       | Prediction     |       | Model Performance|\n",
      "                                      |  (pytest)     |       | Consumption    |       | Monitoring     |\n",
      "                                      +---------------+       +---------------+       +---------------+\n",
      "                                             |                         |\n",
      "                                             |                         |\n",
      "                                             v                         v\n",
      "                                      +---------------+       +---------------+\n",
      "                                      | Deployment    |       | Continuous   |\n",
      "                                      |  (model deployed)|       | Integration    |\n",
      "                                      +---------------+       +---------------+\n",
      "```\n",
      "\n",
      "This block diagram represents the overall architecture of the project, showing the different components and their interactions.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"Block diagram of project architecture\"\n",
    "]\n",
    "\n",
    "# Run queries\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "    \n",
    "    # Retrieve context\n",
    "    retrieved_context = retrieve_context(vector_store, query)\n",
    "    \n",
    "    # Extract context from the retrieved documents\n",
    "    context_value = \"\\n\".join(doc.page_content for doc in retrieved_context)\n",
    "\n",
    "    chain_input = {\n",
    "        \"context\": context_value,\n",
    "        \"question\": query,\n",
    "    }\n",
    "\n",
    "    chain = (\n",
    "\n",
    "         prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    response = chain.invoke(chain_input)\n",
    "    \n",
    "    print(\"Response:\", response)\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
